{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GROUP 1 - GNN and Vertex k -center problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Traditional K-Center Computation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before training our GNN, it is important to develop a framework for generating quality data.  This involves implementing the various algorithms for calculating the k-center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Although we could use approximation algorithms for computing the 1-center, it is actually more efficient to develop a separate algorithm for doing this task.  In the special case of the 1-center, we can actually find a more efficient solution that minimizes the amount of effort taken to reach all of the nodes of the graph.\n",
    "\n",
    "The algorithm below works as follows:\n",
    "1) Compute the distances between all of the vertices of the graph\n",
    "2) Compute the total \"traveling time\" it would take to reach all other vertices in one path from each vertex.  This is just a sum of all of the distances in a single column/row of the distance matrix.\n",
    "3) Iterate through these 'traveling times' to find the vertices that minimize the sums and record them in a list.\n",
    "4) Return that list\n",
    "\n",
    "The algorithm returns all of the possible vertices that satisfy the minimizing conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Single center algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def one_center(G, weighted=False, distance=False):\n",
    "    '''\n",
    "    Function computes distances between all vertices of graph and iterates through edge weights to compute vertices that minimize weights.\n",
    "    Resulting vertices that fit are returned as a list.\n",
    "    Weighted as false reduces computational time marginally.\n",
    "    '''\n",
    "    distances = G.distance_matrix(by_weight=weighted)\n",
    "    sums = sum(distances.columns())\n",
    "    vertices = G.vertices()\n",
    "    centers = []\n",
    "    min_dist = sums[0]\n",
    "    # iterates through each\n",
    "    for vertex in range(len(sums)):\n",
    "        dists = sums[vertex]\n",
    "        if dists < min_dist:\n",
    "            min_dist = dists\n",
    "            centers = [vertex]\n",
    "        elif dists == min_dist:\n",
    "            centers.append(vertex)\n",
    "    if distance:\n",
    "        return [vertices[c] for c in centers], min_dist\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distance traveled if every vertex is to be visited, each time starting at the center: 660\n"
     ]
    },
    {
     "data": {
      "image/png": "fd2461d86ec64f0d6ee3e97edcad01355e2ad140",
      "text/plain": "Graphics Array of size 2 x 2"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = graphs.GridGraph((12, 10))\n",
    "\n",
    "centers, distance = one_center(g, distance=True)\n",
    "\n",
    "print(\"Total distance traveled if every vertex is to be visited, each time starting at the center: \" + str(distance))\n",
    "\n",
    "ga = [g.plot(vertex_colors={'red': [center]}, vertex_labels=False, vertex_size=30) for center in centers]\n",
    "\n",
    "graphics_array(ga, nrows=int(math.sqrt(len(ga))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before implementing an exact solution to the k-center problem, we must create a complete graph, as the minimim dominating set algorithm will not provide accurate results otherwise.  Each edge in the complete graph caputures the distance between the two vertices it connects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def convert_to_complete(G, weighted=False):\n",
    "    '''\n",
    "    Function that converts graph to complete graph. Edge weights are distances between the vertices. \n",
    "    Weighted as false reduced computational time.\n",
    "    '''\n",
    "    all_edges = []\n",
    "    vertices = G.vertices()\n",
    "    # distance is symmetric hence iterating one direction is enough\n",
    "    for i in range(len(vertices)):\n",
    "        for j in range(i + 1, len(vertices)):\n",
    "            all_edges.append((vertices[i], vertices[j], G.distance(vertices[i], vertices[j], by_weight=weighted)))\n",
    "    complete = Graph(all_edges)\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "816e3242d6f450f37598a70bac965b3c2fb407ff",
      "text/plain": "Graphics object consisting of 8 graphics primitives"
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "c378d0fd4c52539523108dde7323c11d1431956f",
      "text/plain": "Graphics object consisting of 31 graphics primitives"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GG23 = graphs.GridGraph((2, 3))\n",
    "GG23.show(vertex_labels=False)\n",
    "\n",
    "convert_to_complete(GG23).show(edge_labels=True, vertex_labels=False, pos=graphs.GridGraph((2, 3)).layout())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This next function exactly solves the k-center problem effectively for 2 or more centers. This method uses the computationally complex dominating set approach to solve for the k centers. We use the minimum dominating set approach, because it is one of the main subproblems of the k-center problem.\n",
    "\n",
    "The basic algorithm is as follows:\n",
    "1) Find and sort each of the possible distances within the inputted graph into a set $D$\n",
    "2) Pick a distance $d \\in D$\n",
    "3) Remove edges in $G$ where $distance > d$\n",
    "4) Find the minimum dominating set $C_{d}$ of this new graph.\n",
    "5) If $|C_{d}| > k$, repeat this process but use $d_{new} \\in D$ where $d_{new} < d$\n",
    "\n",
    "Instead of iterating through every single possible edge distance which is even more computationally expensive, we use a binary search approach to find the optimal edge weight for a k-center solution.  If this function returns a list of k-centers that has less than k elements, the most effective centering is a list of that size.  This means tht adding any other centers will improve the covering, but it is unnecessary as $\\forall v \\in V, distance(v, centers) \\le r_{max}$ where $r_{max}$ is the optimal covering radius.  One important caveat is that the function requires that G is complete and is created by the convert_to_complete function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Basic k-center Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def k_center(G, k=3, distance=False):\n",
    "    '''\n",
    "    Function takes argument graph and modifies into complete graph before implementing efficient k-center algorithm\n",
    "    Detailed description of algorithm can be found above\n",
    "    \n",
    "    '''\n",
    "    # sorted edge list by edge weight\n",
    "    complete = convert_to_complete(G, weighted=True)\n",
    "    weights = sorted(set([edge[2] for edge in complete.edges()]))\n",
    "    high = len(weights) - 1\n",
    "    low = 0\n",
    "    while high - low > 1:\n",
    "        mid = int(math.ceil((high + low) / 2))\n",
    "        r_max = weights[mid]\n",
    "        bottleneck_graph = complete.copy()\n",
    "        # removes all edges from G that have a weight larger than the maximum permitted radius\n",
    "        edges_to_remove = [edge for edge in complete.edges() if edge[2] > r_max]\n",
    "        bottleneck_graph.delete_edges(edges_to_remove)\n",
    "        centers = bottleneck_graph.dominating_set()\n",
    "        # binary search within weights\n",
    "        if len(centers) <= k:\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "    if len(centers) > k:\n",
    "        mid += 1\n",
    "        r_max = weights[mid]\n",
    "        bottleneck_graph = complete.copy()\n",
    "        edges_to_remove = [edge for edge in complete.edges() if edge[2] > r_max]\n",
    "        bottleneck_graph.delete_edges(edges_to_remove)\n",
    "        centers = bottleneck_graph.dominating_set()\n",
    "    if distance:\n",
    "        return centers, r_max\n",
    "    else:\n",
    "        return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Normally, the k-center problem is solved with an approximation algorithm.  There are multiple approximtion algorithms designed for it like the Sh algorithm, Gon algorithm, CDS algorithm; however, the greedy algorithm is simplest to implement and a quick heuristic compared to the \"more exact\" approximations.\n",
    "\n",
    "The greedy algorithm is guaranteed to have an approximation factor of no more than 2x the optimal solution.  For a GNN though, it's more effective to use the exact solution, as the computation time once the model is trained is the same regardless of the initial algorithm used to generate training data.  Further, our GNN benefits, albeit most likely marginally, from an exact algorithm.  The more correct our training data, the more we can guarantee our model's efficacy.  Where this will probably make the largest impact is in larger less regular graphs that might have a difficult to approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Greedy approximation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The basic greedy approximation algorithm works as follows:\n",
    "Consider a graph G = (V, E)\n",
    "1) Pick an arbitrary vertex $c_1 \\in V$ to place into $C$\n",
    "2) Pick the point $c_2 \\in V$ with the largest distance away from $c_1$ and place this into $C$\n",
    "3) Continue this process until $|C| = k$.\n",
    "\n",
    "Since we can precompute distances with the distance_matrix() method from sage, the algorithm is ${\\displaystyle {\\mathcal {O}}(nk)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def k_center_approximation(G, k=3, weighted=False, seed=None, distance=False):\n",
    "    ''' This function uses greedy approximation to find centers.\n",
    "        Returns the resulting k-centers as specified as well as the maximum distance if required.\n",
    "    '''\n",
    "    G.weighted(True)\n",
    "    vertices = G.vertices()\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    starting_index = np.random.randint(len(vertices))\n",
    "    C = [starting_index]\n",
    "    deltas = []\n",
    "    while len(C) < k:\n",
    "        maximized_distance = 0\n",
    "        for v in range(len(vertices)):\n",
    "            if v in C:\n",
    "                continue\n",
    "            dists = [float(G.distance(vertices[v], vertices[c], by_weight=weighted)) for c in C]\n",
    "            min_dist = min(dists)\n",
    "            if min_dist > maximized_distance:\n",
    "                maximized_distance = min_dist\n",
    "                best_candidate = v\n",
    "        deltas.append(maximized_distance)\n",
    "        C.append(best_candidate)\n",
    "    if distance:\n",
    "        return [vertices[c] for c in C], min(deltas)\n",
    "    else:\n",
    "        return [vertices[c] for c in C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the approximation algorithm is approximate, it often doesn't return the purely optimal value for the k-center, as shown below.\n",
    "\n",
    "Here, we calculate the k-centers of two identical graphs using the two different algorithms and show the maximum distance between the centers and any vertex within a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum distance: 4.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "d36769def7b025b5a4758d5c278056a95ac323bb",
      "text/plain": "Graphics object consisting of 32 graphics primitives"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx, dist = k_center_approximation(graphs.GridGraph((4, 5)), k=3, distance=True)\n",
    "\n",
    "print(\"Maximum distance: \" + str(dist)) ; print('\\n')\n",
    "\n",
    "graphs.GridGraph((4, 5)).plot(vertex_colors = {'red': approx}, vertex_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum distance: 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "9cf0845f095b39124af702e5c34f18d73a114f6a",
      "text/plain": "Graphics object consisting of 32 graphics primitives"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = graphs.GridGraph((4, 5))\n",
    "g_complete = convert_to_complete(g)\n",
    "\n",
    "centers, dist = k_center(g_complete, k = 3, distance=True)\n",
    "\n",
    "print(\"Maximum distance: \" + str(dist)) ; print('\\n')\n",
    "g.plot(vertex_colors = {'red': centers}, vertex_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Random Connected Graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Algorithm for creating a random connected graph.  This will be used to train our GNN on general data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def random_connected_graph(n, p, seed=None, weighted=True):\n",
    "    '''\n",
    "        n - number of vertices\n",
    "        p - probability there is an edge between two vertices\n",
    "        uses uniform distribution for edge labeling\n",
    "    '''\n",
    "    G = graphs.RandomGNP(n, p, seed=seed) # ensures that G is completely connected\n",
    "    \n",
    "    sd = seed\n",
    "    while len(G.connected_components()) > 1:\n",
    "        if sd != None:\n",
    "            sd += 1\n",
    "        G = graphs.RandomGNP(n, p, seed=sd)\n",
    "    np.random.seed(seed)\n",
    "    if weighted:\n",
    "        for edge in G.edges():\n",
    "            G.set_edge_label(edge[0], edge[1], RR(np.random.random_sample()))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution\n",
      "Maximum distance to any point:0.49031086432481996\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "a9e752f1d5d4fd7252e4a5d026c66303cd11f51e",
      "text/plain": "Graphics object consisting of 57 graphics primitives"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Center approximation\n",
      "Maximum distance to any point:0.8323777897770746\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "8e5c756aff4b957029b82c909f9d74d6ce5c7f40",
      "text/plain": "Graphics object consisting of 57 graphics primitives"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = random_connected_graph(12, 0.4, seed=2020)\n",
    "g.weighted(True)\n",
    "\n",
    "centers, dist = k_center(g, k=3, distance=True)\n",
    "p_exact = g.plot(layout='circular', vertex_colors={'red': centers}, vertex_labels=False, edge_labels=True)\n",
    "\n",
    "print(\"Exact solution\")\n",
    "print('Maximum distance to any point:' + str(dist)) ; print('\\n')\n",
    "p_exact.show(figsize=15)\n",
    "\n",
    "centers, dist = k_center_approximation(g, k=3, distance=True, weighted=True)\n",
    "p_approximate = g.plot(layout='circular', vertex_colors={'red': centers}, vertex_labels=False, edge_labels=True)\n",
    "\n",
    "print(\"K-Center approximation\")\n",
    "print('Maximum distance to any point:' + str(dist)) ; print('\\n')\n",
    "p_approximate.show(figsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Computational cost visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'runtime')"
      ]
     },
     "execution_count": 12,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "48258ea09e3d90edfadb90796318af614bb64834",
      "text/plain": "<Figure size 2160x360 with 3 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def exact_time(G, k=3):\n",
    "    t_0 = time.time()\n",
    "    centers = k_center(G, k=k, distance=False)\n",
    "    return time.time() - t_0\n",
    "def approx_time(G, k=3):\n",
    "    t_0 = time.time()\n",
    "    centers = k_center_approximation(G, k=k, distance=False, weighted=True)\n",
    "    return time.time() - t_0\n",
    "\n",
    "y_vals = list(range(5,50,5))\n",
    "a1 = [exact_time(random_connected_graph(i,0.5,seed = 2020)) for i in range(5,50,5)]\n",
    "a1_a = [approx_time(random_connected_graph(i,0.5,seed = 2020)) for i in range(5,50,5)]\n",
    "a2 = [exact_time(random_connected_graph(i,0.5,seed = 2020),k=4) for i in range(5,50,5)]\n",
    "a2_a = [approx_time(random_connected_graph(i,0.5,seed = 2020),k=4) for i in range(5,50,5)]\n",
    "a3 = [exact_time(random_connected_graph(i,0.5,seed = 2020),k=5) for i in range(5,50,5)]\n",
    "a3_a = [approx_time(random_connected_graph(i,0.5,seed = 2020),k=5) for i in range(5,50,5)]\n",
    "\n",
    "fig = plt.figure(figsize = (30,5))\n",
    "fig.suptitle('Exact vs Approx Algorithm runtime comparisons (prob = 0.5)',fontsize = 25)\n",
    "\n",
    "fig.add_subplot(131)\n",
    "plt.plot(y_vals,a1,'b-',label = 'exact time')\n",
    "plt.plot(y_vals,a1_a,'r-',label = \"approx time\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('random graph vertices (k = 3)',fontsize = 17)\n",
    "plt.ylabel('runtime',fontsize = 17)\n",
    "\n",
    "\n",
    "fig.add_subplot(132)\n",
    "plt.plot(y_vals,a2,'b-',label = 'exact time')\n",
    "plt.plot(y_vals,a2_a,'r-',label = \"approx time\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('random graph vertices (k = 4)',fontsize = 17)\n",
    "plt.ylabel('runtime',fontsize = 17)\n",
    "\n",
    "fig.add_subplot(133)\n",
    "plt.plot(y_vals,a3,'b-',label = 'exact time')\n",
    "plt.plot(y_vals,a3_a,'r-',label = \"approx time\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('random graph vertices (k = 5)',fontsize = 17)\n",
    "plt.ylabel('runtime',fontsize = 17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To try to find the clusters of data, we simply look in the neighborhoods of each center and classify vertices by their neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import random\n",
    "def clusters(G, centers, weighted=True, as_dict=False):\n",
    "    if not as_dict:\n",
    "        clusts = [0] * G.order()\n",
    "        for vertex in G.vertices():\n",
    "            if vertex in centers:\n",
    "                continue\n",
    "            closest = None\n",
    "            min_dist = 100000000\n",
    "            # we randomly shuffle to not skew our clustering towards the first elements of centers\n",
    "            # otherwise, we get a top-heavy list of clusters which, although is correct, will make classification harder\n",
    "            cs = [*range(len(centers))]\n",
    "            random.shuffle(cs)\n",
    "            for c in cs:\n",
    "                dist = G.distance(centers[c], vertex, by_weight=weighted)\n",
    "                if dist < min_dist:\n",
    "                    closest = c\n",
    "                    min_dist = dist\n",
    "            clusts[vertex] = closest\n",
    "        return clusts\n",
    "    else:\n",
    "        clusts = {center: [] for center in range(len(centers))}\n",
    "        for vertex in G.vertices():\n",
    "            if vertex in centers:\n",
    "                continue\n",
    "            closest = None\n",
    "            min_dist = 100000000\n",
    "            # we randomly shuffle to not skew our clustering towards the first elements of centers\n",
    "            # otherwise, we get a top-heavy list of clusters which, although is correct, will make classification harder\n",
    "            cs = [*range(len(centers))]\n",
    "            random.shuffle(cs)\n",
    "            for c in cs:\n",
    "                dist = G.distance(centers[c], vertex, by_weight=weighted)\n",
    "                if dist < min_dist:\n",
    "                    closest = c\n",
    "                    min_dist = dist\n",
    "            clusts[closest].append(vertex)\n",
    "        return clusts\n",
    "\n",
    "g = random_connected_graph(100, 0.4, seed = 2020)\n",
    "c = clusters(g, k_center_approximation(g, k = 9, distance=False), weighted=True, as_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 2: Building the GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As implementing a GNN efficiently from scratch is beyond our level of expertise, we will be using some functionality of the validated DGL GNN library to quicken production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dgl in /home/user/.local/lib/python3.7/site-packages (0.4.3.post2)\r\n",
      "Requirement already satisfied: networkx>=2.1 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from dgl) (2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy>=1.1.0 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from dgl) (1.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from dgl) (1.16.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from dgl) (2.22.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from networkx>=2.1->dgl) (4.4.1)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (1.25.7)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<2.9,>=2.5 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2019.11.28)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/user/.local/lib/python3.7/site-packages (1.5.0)\r\n",
      "Requirement already satisfied: future in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from torch) (0.17.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /ext/sage/sage-9.0/local/lib/python3.7/site-packages (from torch) (1.16.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# GNN imports\n",
    "!pip3 install --user dgl\n",
    "!pip3 install --user torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from dgl import DGLGraph\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import time\n",
    "RealNumber = float; Integer = int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To be able to train our network, we need to be able to format the graphs for the DGL library.  This first involves converting it to a networkx graph, passing those values into a child class of networkx called DGLGraph, and computing the required information about each individual graph (where the centers are, what are the clusters, what labels are associated with the centers, etc.).  This is made somewhat time inefficient because we are using sage, but it's quick enough to where it's not very noticable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DGLGraph(num_nodes=12, num_edges=46,\n",
       "          ndata_schemes={}\n",
       "          edata_schemes={'weight': Scheme(shape=(), dtype=torch.float32)}),\n",
       " [6, 4, 9],\n",
       " tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]),\n",
       " tensor([True, True, True, True, True, True, True, True, True, True, True, True]),\n",
       " [0, 2, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_DGLGraph(G, k=3, weighted=True):\n",
    "    if weighted:\n",
    "        nxgraph = G.networkx_graph(weight_function = lambda edge: float(edge[2]))\n",
    "    else:\n",
    "        nxgraph = G.networkx_graph(weight_function = lambda edge: float(1))\n",
    "    dgl_G = dgl.DGLGraph()\n",
    "    dgl_G.from_networkx(nxgraph, edge_attrs=['weight'])\n",
    "    centers, distance = k_center_approximation(G, k=k, distance=True)\n",
    "    centers = [int(c) for c in centers]\n",
    "    labels = torch.LongTensor([int(1) if node in centers else int(0) for node in G.vertices()])\n",
    "    mask = torch.BoolTensor([True for _ in range(len(labels))])\n",
    "    clusts = clusters(G, centers, weighted=weighted)\n",
    "    return dgl_G, centers, labels, mask, clusts\n",
    "convert_to_DGLGraph(random_connected_graph(12, 0.4, seed=2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The GAT layer is the backbone of our graph attention network and is based on four key equations.\n",
    "\n",
    "Equation 1 is a linear transformation, implemented with the nn.Linear function from pytorch.\n",
    "\n",
    "Equation 2 calculates the un-normalized attention score $e_{ij}$ using the learned embeddings of adjacent nodes $i$ and $j$.\n",
    "\n",
    "Equation 3 normalizes the attention scores using softmax\n",
    "\n",
    "Equation 4 performs a one-step aggregation of each node's neighbor embeddings using the learned attention as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\\begin{split}\\begin{align}\n",
    "z_i^{(l)}&=W^{(l)}h_i^{(l)},&(1) \\\\\n",
    "e_{ij}^{(l)}&=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)})),&(2)\\\\\n",
    "\\alpha_{ij}^{(l)}&=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})},&(3)\\\\\n",
    "h_i^{(l+1)}&=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right),&(4)\n",
    "\\end{align}\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our GAT uses multiple layers for more accurate classification.  Each attention head has its own parameters and uses the following two equations to merge the information.  Concatenation is used in the middle layers and the average is used in the final layer.\n",
    "\\begin{align}\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This combines all of the structure from the previous two code cells into one GAT class that we can easily call and create our GNN with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before implementing a GNN, we must first convert our graph into a format suitable for a Neural Network.  Typically, GNNs and various other neural networks use real data with structure-independent features.  Since the graphs we are using are purely abstract and have no attributes associated to the nodes, we will approach feature generation differently.  Instead, we will artificially construct features for each node that describe its connectivity in an abstract form. \n",
    "\n",
    "For any arbitrary graph, we want to create an order isomorphic way of describing how far away nodes are within a graph.  Since the direct mapping to a feature vector for node i where $\\overrightarrow{h_i} = \\begin{bmatrix} dist(i, 1) \\\\ dist(i, 2) \\\\. \\\\ . \\\\dist(i, n) \\end{bmatrix}$ only works for identically labeled graphs, we instead denote a feature vector as follows:\n",
    "\n",
    "Consider an arbitrary connected n-node graph $G$.\n",
    "\n",
    "The k-dimensional feature vector (k is an arbitrary dimension somewhat analogous to desired accuracy) is $\\overrightarrow{h_i} = \\begin{bmatrix} h_{i, 1} \\\\ h_{i, 2} \\\\. \\\\ . \\\\ h_{i, k} \\end{bmatrix} \\in \\RR^k$ where $h_{i, j} = |\\{dist(i, j) |  dist(i, j)$ is in the $j^{th}$ equal interval of $[0, Diameter(G)]\\}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 3., 3., 3.],\n",
       "        [0., 3., 4., 3., 1.],\n",
       "        [0., 3., 4., 3., 1.],\n",
       "        [0., 2., 3., 3., 3.],\n",
       "        [0., 3., 3., 3., 2.],\n",
       "        [0., 4., 5., 2., 0.],\n",
       "        [0., 4., 5., 2., 0.],\n",
       "        [0., 3., 3., 3., 2.],\n",
       "        [0., 2., 3., 3., 3.],\n",
       "        [0., 3., 4., 3., 1.],\n",
       "        [0., 3., 4., 3., 1.],\n",
       "        [0., 2., 3., 3., 3.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(G, dim=10, weighted=True):\n",
    "    features = np.zeros((G.order(), dim))\n",
    "    resolution = float(G.diameter(by_weight=weighted)) / dim\n",
    "    vertices = G.vertices()\n",
    "    for i in range(len(vertices)):\n",
    "        for j in range(i + 1, len(vertices)):\n",
    "            distance = G.distance(vertices[i], vertices[j], by_weight=weighted)\n",
    "            section = min(int(math.floor(distance/resolution)), dim - 1)\n",
    "            features[i, section] += 1\n",
    "            features[j, section] += 1\n",
    "    return torch.FloatTensor(features)\n",
    "extract_features(graphs.GridGraph((3, 4)), dim = 5, weighted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Training our GNN on node clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following cell, we start by having our GNN classify the graph into two clusters.  Although this is a small subset of the k-center problems, it is the best candidate to visualize as the output is only two dimensional.  We train this network on 30 random connected graphs of order 50, iterating through each graph only once to save time.  On my (Elia's) system, this took about 2 minutes.  To decrease computation time, you can decrease the amount of graphs to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "weighted = True\n",
    "# creating the graphs to train on\n",
    "G = [random_connected_graph(50, 0.4, weighted=True) for _ in range(30)]\n",
    "# initializing graph features with 'resolution' 30\n",
    "features = [extract_features(graph, dim=30, weighted=True) for graph in G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "k = 2\n",
    "# GNN Creation and training\n",
    "g = []\n",
    "centers = []\n",
    "labels = []\n",
    "masks = []\n",
    "clusts = []\n",
    "for graph in G:\n",
    "    dgl_graph, c, l, m, clsts = convert_to_DGLGraph(graph, k=k, weighted=weighted)\n",
    "    g.append(dgl_graph)\n",
    "    centers.append(c)\n",
    "    labels.append(l)\n",
    "    masks.append(m)\n",
    "    clusts.append(torch.LongTensor(clsts))\n",
    "\n",
    "# creating the network\n",
    "net_2_clustering = GAT(g[0],\n",
    "      in_dim=features[0].size()[1],\n",
    "      hidden_dim=k*2,\n",
    "      out_dim=2,\n",
    "      num_heads=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_2_clustering.parameters(), lr=1e-3)\n",
    "\n",
    "duration = []\n",
    "losses = []\n",
    "count = 0\n",
    "for k in range(1):\n",
    "    for i in range(len(g)):\n",
    "        for epoch in range(25):\n",
    "            count += 1\n",
    "            if epoch >= 3:\n",
    "                t0 = time.time()\n",
    "\n",
    "            logits = net_2_clustering(features[i])\n",
    "            logp = F.log_softmax(logits, 1)\n",
    "            loss = F.nll_loss(logp, clusts[i])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch >= 3:\n",
    "                duration.append(time.time() - t0)\n",
    "                losses.append(loss.item())\n",
    "            # print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            #    count, loss.item(), np.mean(duration)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "a5506dfdc4f09926677b0246f31bd89d4dc65b82",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot([*range(len(losses))], losses)\n",
    "plt.title(\"Clustering-Based GNN Loss for k=2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Visualization of Clustering-GNN output for k=2')"
      ]
     },
     "execution_count": 388,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "5058999accdf06b01ddc45c66b4828cddb75ec5b",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = random_connected_graph(50, 0.4, weighted=True)\n",
    "test_features = extract_features(test_graph, dim=30, weighted=True)\n",
    "test_centers = k_center_approximation(test_graph, k=2, distance=False)\n",
    "test_clusters = clusters(test_graph, test_centers, weighted=True, as_dict=True)\n",
    "results = net_2_clustering(test_features).detach().numpy()\n",
    "plt.scatter(results[:, 0], results[:, 1])\n",
    "plt.scatter(results[test_centers, 0], results[test_centers, 1], c='red')\n",
    "plt.title(\"Visualization of Clustering-GNN output for k=2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can generalize this approach to however many clusters/k-centers we want.  In this case, we try training our network on 6 centers/clusters (This is so that there are enough colors to display the clusters in the visualizations; aside from that the choice of k is arbitrary as long as its larger than 2).  Although the same algorithm/approach applies, this network now returns an 6-dimensional vector which is much more difficult to visualize.  We try to combat this through two dimension reduction techniques: TSNE (T-distributed Stochastic Neighbor Embedding) and PCA (Principle Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "k = 6\n",
    "\n",
    "g = []\n",
    "centers = []\n",
    "labels = []\n",
    "masks = []\n",
    "clusts = []\n",
    "for graph in G:\n",
    "    dgl_graph, c, l, m, clsts = convert_to_DGLGraph(graph, k=k, weighted=weighted)\n",
    "    g.append(dgl_graph)\n",
    "    centers.append(c)\n",
    "    labels.append(l)\n",
    "    masks.append(m)\n",
    "    clusts.append(torch.LongTensor(clsts))\n",
    "\n",
    "# creating the network\n",
    "net_6_clustering = GAT(g[0],\n",
    "      in_dim=features[0].size()[1],\n",
    "      hidden_dim=k*2,\n",
    "      out_dim=k,\n",
    "      num_heads=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_6_clustering.parameters(), lr=1e-3)\n",
    "\n",
    "duration = []\n",
    "losses = []\n",
    "count = 0\n",
    "for k in range(1):\n",
    "    for i in range(len(g)):\n",
    "        for epoch in range(25):\n",
    "            count += 1\n",
    "            if epoch >= 3:\n",
    "                t0 = time.time()\n",
    "\n",
    "            logits = net_6_clustering(features[i])\n",
    "            logp = F.log_softmax(logits, 1)\n",
    "            loss = F.nll_loss(logp, clusts[i])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch >= 3:\n",
    "                duration.append(time.time() - t0)\n",
    "                losses.append(loss.item())\n",
    "            # print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            #    count, loss.item(), np.mean(duration)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "18c0885b28a374dfd153250615e42d0d84b62ec7",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot([*range(len(losses))], losses)\n",
    "plt.title(\"Clustering-Based GNN Loss for k=6\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "test_graph = random_connected_graph(50, 0.4, weighted=True)\n",
    "test_features = extract_features(test_graph, dim=30, weighted=True)\n",
    "test_centers = k_center_approximation(test_graph, k=6, distance=False)\n",
    "test_clusters = clusters(test_graph, test_centers, weighted=True, as_dict=True)\n",
    "results = net_6_clustering(test_features).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "9b92da34adf1c8550370e75439d84e428cc81f6e",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.6287686 0.1723657]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "colors = ['blue', 'green', 'black', 'cyan', 'magenta', 'yellow', 'red']\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(results)\n",
    "dim1 = pca_result[:,0]\n",
    "dim2 = pca_result[:,1]\n",
    "# plt.scatter(dim1, dim2)\n",
    "# For some reason plotting in a for loop doesn't work, so we just repeat the code for each cluster\n",
    "plt.scatter(dim1[test_clusters[0]], dim2[test_clusters[0]], c=colors[0])\n",
    "plt.scatter(dim1[test_clusters[1]], dim2[test_clusters[1]], c=colors[1])\n",
    "plt.scatter(dim1[test_clusters[2]], dim2[test_clusters[2]], c=colors[2])\n",
    "plt.scatter(dim1[test_clusters[3]], dim2[test_clusters[3]], c=colors[3])\n",
    "plt.scatter(dim1[test_clusters[4]], dim2[test_clusters[4]], c=colors[4])\n",
    "plt.scatter(dim1[test_clusters[5]], dim2[test_clusters[5]], c=colors[5])\n",
    "plt.scatter(dim1[test_centers], dim2[test_centers], c='red')\n",
    "plt.title(\"Visualization of Clustering-GNN output for k=6 with PCA\")\n",
    "plt.show()\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 46 nearest neighbors...\n",
      "[t-SNE] Indexed 50 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 50 samples in 0.003s...\n",
      "[t-SNE] Computed conditional probabilities for sample 50 / 50\n",
      "[t-SNE] Mean sigma: 0.245103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.039890\n",
      "[t-SNE] KL divergence after 300 iterations: 1.332662\n",
      "t-SNE done! Time elapsed: 0.12821030616760254 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "d33b75b289e35ad4a2edb9fa94d5aba25586735c",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t0 = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=15, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(results)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-t0))\n",
    "dim1 = tsne_results[:, 0]\n",
    "dim2 = tsne_results[:, 1]\n",
    "# plt.scatter(dim1, dim2)\n",
    "plt.scatter(dim1[test_clusters[0]], dim2[test_clusters[0]], c=colors[0])\n",
    "plt.scatter(dim1[test_clusters[1]], dim2[test_clusters[1]], c=colors[1])\n",
    "plt.scatter(dim1[test_clusters[2]], dim2[test_clusters[2]], c=colors[2])\n",
    "plt.scatter(dim1[test_clusters[3]], dim2[test_clusters[3]], c=colors[3])\n",
    "plt.scatter(dim1[test_clusters[4]], dim2[test_clusters[4]], c=colors[4])\n",
    "plt.scatter(dim1[test_clusters[5]], dim2[test_clusters[5]], c=colors[5])\n",
    "plt.scatter(dim1[test_centers], dim2[test_centers], c='red')\n",
    "plt.title(\"Visualization of Clustering-GNN output for k=6 with TSNE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Training our network on centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, we approach the k-center problem from the original angle.  We train our network to classify vertices depending on whether or not they are centers.  The algorithm itself is virtually unchanged.  This, unfortunately, relies on relatively sparse data, as the vertices often outnumber the amount of centers in realistic applications.  Still, we can try to extract valuable information from this approach, mainly stemming from the 2-dimensional output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "k = 10\n",
    "\n",
    "g = []\n",
    "centers = []\n",
    "labels = []\n",
    "masks = []\n",
    "clusts = []\n",
    "for graph in G:\n",
    "    dgl_graph, c, l, m, clsts = convert_to_DGLGraph(graph, k=k, weighted=weighted)\n",
    "    g.append(dgl_graph)\n",
    "    centers.append(c)\n",
    "    labels.append(l)\n",
    "    masks.append(m)\n",
    "    clusts.append(torch.LongTensor(clsts))\n",
    "\n",
    "# creating the network\n",
    "net_centers = GAT(g[0],\n",
    "      in_dim=features[0].size()[1],\n",
    "      hidden_dim=10,\n",
    "      out_dim=2,\n",
    "      num_heads=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net_centers.parameters(), lr=1e-3)\n",
    "\n",
    "duration = []\n",
    "losses = []\n",
    "count = 0\n",
    "for k in range(1):\n",
    "    for i in range(len(g)):\n",
    "        for epoch in range(25):\n",
    "            count += 1\n",
    "            if epoch >= 3:\n",
    "                t0 = time.time()\n",
    "\n",
    "            logits = net_centers(features[i])\n",
    "            logp = F.log_softmax(logits, 1)\n",
    "            loss = F.nll_loss(logp, labels[i])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch >= 3:\n",
    "                duration.append(time.time() - t0)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            #    count, loss.item(), np.mean(duration)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "5b052076e68198949fd97278ef265fe06aeb416d",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot([*range(len(losses))], losses)\n",
    "plt.title(\"Center-Based GNN Loss for k=10\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 3, 4, 9, 11, 28, 36, 0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "070dc65fe05e39bb747c5c76311d4ba9f3c6d9ee",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph = random_connected_graph(50, 0.4, weighted=True)\n",
    "test_features = extract_features(test_graph, dim=30, weighted=True)\n",
    "test_centers = k_center_approximation(test_graph, k=10, distance=False)\n",
    "print(test_centers)\n",
    "results = net_centers(test_features).detach().numpy()\n",
    "plt.scatter(results[:, 0], results[:, 1])\n",
    "plt.scatter(results[test_centers, 0], results[test_centers, 1], c='red')\n",
    "plt.title(\"Visualization of Center-GNN output for k=10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Instance of GNN with features clearly defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To see how effective this approach is on real world data compared to our problem, we implemented one of the original paper's GNN's for the cora citation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    mask = torch.BoolTensor(data.train_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    # torch.set_printoptions(threshold=5000)\n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 4,  ..., 1, 0, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext/sage/sage-9.0/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/ext/sage/sage-9.0/local/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9464 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9445 | Time(s) nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002 | Loss 1.9426 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.9407 | Time(s) 0.1270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004 | Loss 1.9388 | Time(s) 0.1230\n",
      "Epoch 00005 | Loss 1.9368 | Time(s) 0.1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006 | Loss 1.9349 | Time(s) 0.1138\n",
      "Epoch 00007 | Loss 1.9330 | Time(s) 0.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008 | Loss 1.9310 | Time(s) 0.1144\n",
      "Epoch 00009 | Loss 1.9291 | Time(s) 0.1132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010 | Loss 1.9271 | Time(s) 0.1122\n",
      "Epoch 00011 | Loss 1.9252 | Time(s) 0.1115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012 | Loss 1.9232 | Time(s) 0.1118\n",
      "Epoch 00013 | Loss 1.9212 | Time(s) 0.1115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014 | Loss 1.9193 | Time(s) 0.1110\n",
      "Epoch 00015 | Loss 1.9173 | Time(s) 0.1107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016 | Loss 1.9153 | Time(s) 0.1101\n",
      "Epoch 00017 | Loss 1.9133 | Time(s) 0.1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018 | Loss 1.9112 | Time(s) 0.1096\n",
      "Epoch 00019 | Loss 1.9092 | Time(s) 0.1097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020 | Loss 1.9072 | Time(s) 0.1096\n",
      "Epoch 00021 | Loss 1.9051 | Time(s) 0.1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00022 | Loss 1.9030 | Time(s) 0.1111\n",
      "Epoch 00023 | Loss 1.9009 | Time(s) 0.1105\n",
      "Epoch 00024 | Loss 1.8988 | Time(s) 0.1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00025 | Loss 1.8967 | Time(s) 0.1100\n",
      "Epoch 00026 | Loss 1.8946 | Time(s) 0.1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027 | Loss 1.8925 | Time(s) 0.1096\n",
      "Epoch 00028 | Loss 1.8903 | Time(s) 0.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00029 | Loss 1.8881 | Time(s) 0.1096\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, mask = load_cora_data()\n",
    "data = citegrh.load_cora()\n",
    "\n",
    "# create the model, 2 heads, each head has hidden size 8\n",
    "net = GAT(g,\n",
    "          in_dim=features.size()[1],\n",
    "          hidden_dim=8,\n",
    "          out_dim=7,\n",
    "          num_heads=2)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "print(labels)\n",
    "# main loop\n",
    "dur = []\n",
    "for epoch in range(30):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[mask], labels[mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### This is a visualization of the attention learned for each edge on the CORA dataset\n",
    "![](https://data.dgl.ai/tutorial/gat/cora-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 1,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}